"""
PrintDambo Module - Python Port (Optimized)
Main module for insurance terms document generation.
Contains the core logic for reading PGM, copying terms, and revising documents.

OPTIMIZATION: Uses Pandas DataLoader for bulk data reading (10-50x faster than COM)
"""
import os
import datetime
import numpy as np
from typing import Optional, List, Dict, Any, Tuple

from .word_utils import WordHandler, wdCollapseEnd, wdCollapseStart, wdSectionBreakOddPage
from .data_loader import DataLoader, ExcelWriter
from .tag_processor import TagProcessor, TagContext, create_tag_context_from_dambo_att


class DamboAttributes:
    """Data class to hold coverage attributes."""
    def __init__(self):
        self.ë‹´ë³´ì½”ë“œ = ""
        self.ëŒ€í‘œë‹´ë³´ì½”ë“œ = ""
        self.ì§„ë‹¨í™•ì • = 0
        self.ë¶€ëª¨ = 0
        self.ì˜ˆì•½ê°€ì…ì—°ë ¹ = 0
        self.ëª¨ë“ˆ = ""
        self.í˜•êµ¬ë¶„ = ""
        self.ë‹¨ì²´ = 0
        self.ìë™ê°±ì‹ í˜• = 0
        self.ë©´ì±…ê°•ì œë°˜ì˜ = ""
        self.ë©´ì±… = 0
        self.ê°ì•¡ = 0
        self.ì—°ì¥í˜• = 0
        
        # ë…ë¦½íŠ¹ì•½
        self.ë…íŠ¹ê°ì•¡ì—¬ë¶€ = ""
        self.ë…íŠ¹ë©´ì±…ì—¬ë¶€ = ""
        
        # ì„¸ë¶€ë³´ì¥ëª… (for {ì„¸ë¶€ë³´ì¥N} tag replacement)
        self.ì„¸ë¶€ë³´ì¥ëª… = ""
        self.ì„¸ë¶€ë³´ì¥ëª…_list = []  # ì¶œë ¥ë‹´ë³´ëª…ì´ ê°™ì€ ë‹´ë³´ë“¤ì˜ ë‹´ë³´ëª… ë¦¬ìŠ¤íŠ¸
        
        # ë…ë¦½íŠ¹ì•½ ì„¤ì •
        self.ë…ë¦½íŠ¹ì•½ = 0


class PrintDambo:
    """
    Main class for insurance terms document generation.
    Uses Pandas-based DataLoader for high-performance data access.
    """
    
    def __init__(self, data_loader: DataLoader):
        """
        Initialize with DataLoader instance.
        
        Args:
            data_loader: Pre-loaded DataLoader with config and PGM data
        """
        self.data = data_loader
        self.word = WordHandler()
        self.tag_processor = TagProcessor()  # Tag processing module
        
        # Column Locations (cached from data arrays)
        self.loc_í™•ì¥ë²ˆí˜¸ = 0
        self.loc_ë³´í—˜ê¸°ê°„ì—°ì¥í˜• = 0
        self.loc_ê°„í¸ê³ ì§€ = 0
        self.loc_í†µí•©ê³ ì§€ = 0
        self.loc_íƒœì•„êµ¬ë¶„ = 0
        self.loc_ë³´ê¸°ë‚©ê¸° = 0
        self.loc_ë‹´ë³´ê·¸ë£¹ = 0
        
        # ë³´ì¥êµ¬ì¡° columns
        self.loc_ë³´ì¥ë°°ìˆ˜ = 0
        self.loc_ì„¸ë¶€ë‹´ë³´ìˆœë²ˆ = 0
        self.loc_ì„¸ë¶€ë‹´ë³´ì½”ë“œ = 0
        self.loc_ê¸‰ë¶€íƒˆí‡´ìœ¨ê°œìˆ˜ = 0
        self.loc_ë‚©ë©´íƒˆí‡´ìœ¨ê°œìˆ˜ = 0
        
        # ë³´ì¥ë°°ìˆ˜ columns
        self.loc_ì§€ê¸‰ë¥  = 0
        self.loc_ë¶„í• ì§€ê¸‰ì°¨ë…„ = 0
        self.loc_ì—°ì§€ê¸‰íšŸìˆ˜ = 0
        self.loc_ë©´ì±…ê¸°ê°„ = 0
        self.loc_ê°ì•¡ê¸°ê°„ = 0
        self.loc_ê°ì•¡ê¸°ê°„2 = 0
        self.loc_ê°ì•¡ë¹„ìœ¨ = 0
        self.loc_ê°ì•¡ë¹„ìœ¨2 = 0
        self.loc_15ì„¸ë©´ì±… = 0
        
        # ì„¸ë¶€ë‹´ë³´ ê´€ë ¨
        self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ = 0
        self.ì„¸ë¶€ë‹´ë³´_bencoef = []
        self.ì„¸ë¶€ë‹´ë³´ì½”ë“œList = []
        self.m_point = 0
        self.min_riskrate_row = 0
        self.ë‚©ì…ë©´ì œì—¬ë¶€ = False
        
        # ê°ì•¡/ë©´ì±… ê´€ë ¨
        self.isê°ì•¡ë‘ë²ˆ = False
        self.sum_ë©´ì±… = 0
        self.sum_ê°ì•¡ = 0
        
        # ë³´ê¸°ë‚©ê¸° ê´€ë ¨
        self.n_array = []
        self.age_b = 0
        self.age_e = 0
        self.term_lookup_key = ""
        
        # Paths (from data_loader)
        self.ì¶œë ¥íŒŒì¼ëª… = ""
        
        # Copied terms tracking
        self.copied_list_strings = set()
        
        # Number marks â‘  â‘¡ â‘¢ etc.
        self.num_mark_arr = ["â‘ ", "â‘¡", "â‘¢", "â‘£", "â‘¤", "â‘¥", "â‘¦", "â‘§", "â‘¨", "â‘©", 
                            "â‘ª", "â‘«", "â‘¬", "â‘­", "â‘®"]
        
        # ë…ë¦½íŠ¹ì•½ ê´€ë ¨
        self.is_ë…íŠ¹ê°„í¸ = False
        self.ë…íŠ¹ë‹´ë³´ê·¸ë£¹ = 0
        self.ë…ë¦½íŠ¹ì•½ëª… = ""

        # Source files and documents
        self.base_files = {}
        self.csv_loader = None
        self.source_docs = {}
        self.ì¶œë ¥ë‹´ë³´ëª…_groups = {}

        # Excel Writer for result saving (lazy init)
        self._excel_writer = None

    def execute(self, log_callback=None, progress_callback=None):
        """
        Main entry point for terms generation.
        Uses template data (coverage_list) for mapping results and PGM data.
        """
        try:
            if log_callback:
                log_callback("ì´ˆê¸°í™” ë° ì»¬ëŸ¼ ìœ„ì¹˜ íŒŒì•…...")
            
            # Initialize Word with performance settings
            self.word.start_app(visible=False)
            
            # Find column locations from cached arrays
            self._find_column_locations(log_callback)
            
            # Get coverage list from template_data
            template_data = getattr(self.data, 'template_data', {}) or {}
            coverage_list = template_data.get('coverage_list', [])
            
            if not coverage_list:
                if log_callback:
                    log_callback("âŒ ë‹´ë³´ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤. í…œí”Œë¦¿ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
                return
            
            total_steps = len(coverage_list)
            
            if log_callback:
                log_callback(f"ì¢…ì†íŠ¹ì•½ ì•½ê´€ìƒì„±: {total_steps}ê°œ í•­ëª©...")
            
            # Generate output filename
            now = datetime.datetime.now()
            self.ì¶œë ¥íŒŒì¼ëª… = f"03_0_ì•½ê´€_{self.data.prod_name}_{now.strftime('%Y%m%d')}_{now.strftime('%H%M')}.docx"
            
            # ============ PRE-OPEN DOCUMENTS FOR PERFORMANCE ============
            # Open target document ONCE before loop
            target_path = getattr(self.data, 'product_doc_file', None)
            if target_path and os.path.exists(target_path):
                if log_callback:
                    log_callback(f"ğŸ“„ íƒ€ê²Ÿ ë¬¸ì„œ ì—´ê¸°: {os.path.basename(target_path)}")
                self.word.target_doc = self.word.open_doc(target_path)
            
            # Cache source documents (open once, reuse)
            self.source_docs = {}  # {file_type: doc}
            if hasattr(self, 'base_files') and self.base_files:
                if log_callback:
                    log_callback(f"ğŸ“„ ì†ŒìŠ¤ ë¬¸ì„œ {len(self.base_files)}ê°œ ì—´ê¸°...")
                for file_type, file_path in self.base_files.items():
                    doc = self.word.open_doc(file_path)
                    if doc:
                        self.source_docs[file_type] = doc
            
            # ============ OPTIMIZE WORD FOR BATCH OPERATIONS ============
            # Disable spell check, grammar check, pagination for massive speed boost
            self.word.optimize_for_batch_operations()
            if log_callback:
                log_callback("âš¡ Word ìµœì í™” ì„¤ì • ì ìš© (ë§ì¶¤ë²•/ë¬¸ë²•ê²€ì‚¬/í˜ì´ì§€ ê³„ì‚° ë¹„í™œì„±í™”)")
            
            # ============ COMMENT CACHE FOR PERFORMANCE ============
            # Build comment caches ONCE instead of traversing all comments per coverage
            self._source_comment_cache = {}  # {ëŒ€í‘œë‹´ë³´ì½”ë“œ: {source_type: (copy_start, copy_end, íŠ¹ë³„ì•½ê´€ëª…)}}
            self._target_comment_objects = []  # [(comment_text, comment_COM_object), ...] - COM refs auto-update positions
            self._build_comment_caches(log_callback)
            
            if log_callback:
                log_callback("âœ… ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ\n")
            
            # ============ CONNECT COMPONENTS ============
            # Connect CSVLoader to TagProcessor if available
            if hasattr(self, 'csv_loader') and self.csv_loader:
                self.tag_processor.csv_loader = self.csv_loader
                if log_callback:
                    log_callback("âœ… TagProcessorì— CSVLoader ì—°ê²° ì™„ë£Œ")
            
            # ============ GROUP BY ì¶œë ¥ë‹´ë³´ëª… FOR ì„¸ë¶€ë³´ì¥ TAGS ============
            # VBA: ì¶œë ¥ë‹´ë³´ëª…ì´ ê°™ì€ ë‹´ë³´ë“¤ì€ {ì„¸ë¶€ë³´ì¥1}, {ì„¸ë¶€ë³´ì¥2}... ìˆœë²ˆìœ¼ë¡œ ì¹˜í™˜
            # ì •ë ¬ ê¸°ì¤€: ë‹´ë³´ì½”ë“œ ì˜¤ë¦„ì°¨ìˆœ
            self.ì¶œë ¥ë‹´ë³´ëª…_groups = {}  # {ì¶œë ¥ë‹´ë³´ëª…: [(ë‹´ë³´ì½”ë“œ, ë‹´ë³´ëª…), ...]}
            for coverage in coverage_list:
                ì¶œë ¥ë‹´ë³´ëª… = str(coverage.get('ì¶œë ¥ë‹´ë³´ëª…', '') or 
                              coverage.get('ë‹´ë³´ëª…_ì¶œë ¥ë¬¼ëª…ì¹­', '') or '').strip()
                ë‹´ë³´ëª… = str(coverage.get('ë‹´ë³´ëª…', '') or 
                           coverage.get('ì„¸ë¶€ë³´ì¥ëª…', '') or '').strip()
                ë‹´ë³´ì½”ë“œ = str(coverage.get('ë‹´ë³´ì½”ë“œ', '')).strip()
                
                if ì¶œë ¥ë‹´ë³´ëª… and ë‹´ë³´ëª…:
                    if ì¶œë ¥ë‹´ë³´ëª… not in self.ì¶œë ¥ë‹´ë³´ëª…_groups:
                        self.ì¶œë ¥ë‹´ë³´ëª…_groups[ì¶œë ¥ë‹´ë³´ëª…] = []
                    self.ì¶œë ¥ë‹´ë³´ëª…_groups[ì¶œë ¥ë‹´ë³´ëª…].append((ë‹´ë³´ì½”ë“œ, ë‹´ë³´ëª…))
            
            if log_callback and self.ì¶œë ¥ë‹´ë³´ëª…_groups:
                log_callback(f"ğŸ“‹ ì¶œë ¥ë‹´ë³´ëª… ê·¸ë£¹: {len(self.ì¶œë ¥ë‹´ë³´ëª…_groups)}ê°œ")
            
            # ========================= PHASE 1: COPY & PROCESS TERMS =========================
            if log_callback:
                log_callback(f"\nğŸ“„ Phase 1: ì•½ê´€ ë³µì‚¬ ë° íƒœê·¸ ì²˜ë¦¬ ({total_steps}ê°œ)...")

            # ===== PRE-COMPUTE: ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ê³„ì‚° (ë£¨í”„ë‹¹ getattr ì œê±°) =====
            cached_ë‹¨ì²´ë³´í—˜ = getattr(self.data, 'ë‹¨ì²´ë³´í—˜', 0)
            cached_ìë™ê°±ì‹ í˜• = getattr(self.data, 'ìë™ê°±ì‹ í˜•', 0)
            cached_ë…ë¦½íŠ¹ì•½ = getattr(self.data, 'ë…ë¦½íŠ¹ì•½', 0)
            has_base_files = hasattr(self, 'base_files') and self.base_files
            has_source_doc = self.data.arr_source_doc is not None

            # Pre-compute base_files fallback
            base_files_fallback_path = ""
            base_files_fallback_name = ""
            if has_base_files and self.base_files:
                first_file = list(self.base_files.values())[0]
                base_files_fallback_path = os.path.dirname(first_file)
                base_files_fallback_name = os.path.basename(first_file)

            # Pre-sort ì¶œë ¥ë‹´ë³´ëª…_groups (avoid re-sorting each iteration)
            sorted_groups = {}
            for group_name, group_items in self.ì¶œë ¥ë‹´ë³´ëª…_groups.items():
                sorted_items = sorted(group_items, key=lambda x: x[0])
                sorted_groups[group_name] = [item[1] for item in sorted_items]

            # Pre-compute source doc lookup dict
            source_doc_lookup = {}
            if has_source_doc and self.data.arr_source_doc is not None:
                for j in range(len(self.data.arr_source_doc)):
                    key = str(self.data.arr_source_doc[j][0]).strip()
                    source_doc_lookup[key] = str(self.data.arr_source_doc[j][1]).strip()

            # ===== MAIN LOOP =====
            for i, coverage in enumerate(coverage_list):
                dambo_code = str(coverage.get('ë‹´ë³´ì½”ë“œ', '')).strip()
                dambo_category = str(coverage.get('êµ¬ë¶„ê°’', '')).strip()
                ëŒ€í‘œë‹´ë³´ì½”ë“œ = str(coverage.get('ëŒ€í‘œë‹´ë³´ì½”ë“œ', '')).strip()

                if log_callback:
                    if (i + 1) % 20 == 0 or i == 0 or (i + 1) == total_steps:
                        log_callback(f"  [{i+1}/{total_steps}] ì²˜ë¦¬ì¤‘: {dambo_code} ({ëŒ€í‘œë‹´ë³´ì½”ë“œ})")

                # Create DamboAttributes (using pre-computed values)
                dambo_att = DamboAttributes()
                dambo_att.ë‹´ë³´ì½”ë“œ = dambo_code
                dambo_att.ëŒ€í‘œë‹´ë³´ì½”ë“œ = ëŒ€í‘œë‹´ë³´ì½”ë“œ
                dambo_att.ë©´ì±… = int(coverage.get('ë©´ì±…', 0) or 0)
                dambo_att.ê°ì•¡ = int(coverage.get('ê°ì•¡', 0) or 0)
                dambo_att.ì—°ì¥í˜• = int(coverage.get('ì—°ì¥í˜•', 0) or 0)
                dambo_att.í˜•êµ¬ë¶„ = str(coverage.get('í˜•êµ¬ë¶„', '')).strip()

                ëª¨ë“ˆ_val = coverage.get('ëª¨ë“ˆ', '')
                dambo_att.ëª¨ë“ˆ = f"{ëª¨ë“ˆ_val}ëª¨ë“ˆ" if ëª¨ë“ˆ_val else ""
                dambo_att.ë‹¨ì²´ = cached_ë‹¨ì²´ë³´í—˜
                dambo_att.ìë™ê°±ì‹ í˜• = cached_ìë™ê°±ì‹ í˜•
                dambo_att.ì§„ë‹¨í™•ì • = int(coverage.get('ì§„ë‹¨í™•ì •', 0) or 0)
                dambo_att.ë¶€ëª¨ = int(coverage.get('ë¶€ëª¨', 0) or 0)
                dambo_att.ì˜ˆì•½ê°€ì…ì—°ë ¹ = int(coverage.get('ì˜ˆì•½ê°€ì…ì—°ë ¹', 0) or 0)
                dambo_att.ë…ë¦½íŠ¹ì•½ = cached_ë…ë¦½íŠ¹ì•½

                dambo_att.ì„¸ë¶€ë³´ì¥ëª… = str(coverage.get('ì„¸ë¶€ë³´ì¥ëª…', '') or
                                         coverage.get('ë‹´ë³´ëª…', '') or '').strip()

                ì¶œë ¥ë‹´ë³´ëª… = str(coverage.get('ì¶œë ¥ë‹´ë³´ëª…', '') or
                              coverage.get('ë‹´ë³´ëª…_ì¶œë ¥ë¬¼ëª…ì¹­', '') or '').strip()
                dambo_att.ì„¸ë¶€ë³´ì¥ëª…_list = sorted_groups.get(ì¶œë ¥ë‹´ë³´ëª…, [dambo_att.ì„¸ë¶€ë³´ì¥ëª…])

                # Read PGM Data
                self._read_pgm_loop(dambo_code, dambo_att, i, log_callback)

                if cached_ë…ë¦½íŠ¹ì•½ != 0:
                    self._set_ë…íŠ¹_file_path(dambo_att, ëŒ€í‘œë‹´ë³´ì½”ë“œ, dambo_code, log_callback)

                # Find source document info (pre-computed lookups)
                íŠ¹ë³„ì•½ê´€ê²½ë¡œ = ""
                íŠ¹ë³„ì•½ê´€íŒŒì¼ëª… = ""

                if has_base_files:
                    if dambo_category in self.base_files:
                        íŠ¹ë³„ì•½ê´€file = self.base_files[dambo_category]
                        íŠ¹ë³„ì•½ê´€ê²½ë¡œ = os.path.dirname(íŠ¹ë³„ì•½ê´€file)
                        íŠ¹ë³„ì•½ê´€íŒŒì¼ëª… = os.path.basename(íŠ¹ë³„ì•½ê´€file)
                    else:
                        íŠ¹ë³„ì•½ê´€ê²½ë¡œ = base_files_fallback_path
                        íŠ¹ë³„ì•½ê´€íŒŒì¼ëª… = base_files_fallback_name
                elif dambo_category in source_doc_lookup:
                    íŠ¹ë³„ì•½ê´€ê²½ë¡œ = self.data.ì¢…ì†íŠ¹ì•½ê²½ë¡œ
                    íŠ¹ë³„ì•½ê´€íŒŒì¼ëª… = source_doc_lookup[dambo_category]

                # Copy terms AND identify the range where it was pasted
                target_range = self._copy_terms(ëŒ€í‘œë‹´ë³´ì½”ë“œ, íŠ¹ë³„ì•½ê´€ê²½ë¡œ, íŠ¹ë³„ì•½ê´€íŒŒì¼ëª…,
                                              dambo_category, dambo_att, log_callback)

                # Process Tags IMMEDIATELY on the target range
                if target_range:
                    self._revise_terms(dambo_att, i, target_range, log_callback)

                # Update progress
                if progress_callback:
                    progress_callback(int((i + 1) / total_steps * 100))

            
            # PHASE 2 REMOVED - Tags are processed in Phase 1 loop
            pass
            
            # Final processing - 0ì„¸ìë…€ íƒœê·¸ ì²˜ë¦¬
            self._revise_ë³„í‘œ_0ì„¸(log_callback)

            # ============ RESTORE WORD SETTINGS ============
            # Restore spell check, grammar check, pagination before saving
            self.word.restore_after_batch()
            
            # Enable screen updating to show final result
            self.word.enable_screen_updating(True)
            
            if log_callback:
                log_callback("\nâœ… ì•½ê´€ ìƒì„± ì™„ë£Œ (ì €ì¥ ëŒ€ê¸°)")
                
        except Exception as e:
            if log_callback:
                log_callback(f"Error: {e}")
            import traceback
            traceback.print_exc()

    def save_output(self, log_callback=None):
        """Save the modified document to output path."""
        output_file = getattr(self.data, 'output_file', None)
        if log_callback:
            log_callback(f"\nğŸ“‹ ì €ì¥ ì¤€ë¹„ ì¤‘...")
            log_callback(f"   ì¶œë ¥ ê²½ë¡œ: {output_file}")
            # log_callback(f"   target_doc ì¡´ì¬: {self.word.target_doc is not None}")
        
        if not output_file:
            if log_callback:
                log_callback("âŒ ì €ì¥ ì‹¤íŒ¨: ì¶œë ¥ íŒŒì¼ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        elif not self.word.target_doc:
            if log_callback:
                log_callback("âŒ ì €ì¥ ì‹¤íŒ¨: ëŒ€ìƒ Word ë¬¸ì„œê°€ ì—´ë ¤ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
                log_callback("   ì›ì¸: _copy_termsì—ì„œ target_docì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            try:
                self.word.target_doc.SaveAs2(output_file)
                if log_callback:
                    log_callback(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            except Exception as save_err:
                if log_callback:
                    log_callback(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {save_err}")

    def close(self):
        """Close Word documents."""
        self.word.close_all()

    def _find_column_locations(self, log_callback=None):
        """
        Find column locations in PGM arrays.
        Uses cached numpy arrays for fast lookup.
        """
        # Find column locations in Main sheet
        if self.data.arr_pgm_main is not None and len(self.data.arr_pgm_main) > 1:
            header_row = self.data.arr_pgm_main[1]  # Assuming header is row 1
            
            for i, val in enumerate(header_row):
                attr_name = str(val or "").strip().replace("\n", "")
                
                if attr_name == "ExpansionNumber":
                    self.loc_í™•ì¥ë²ˆí˜¸ = i
                elif attr_name == "ë³´í—˜ê¸°ê°„ì—°ì¥í˜•":
                    self.loc_ë³´í—˜ê¸°ê°„ì—°ì¥í˜• = i
                elif attr_name == "ZA_ConvenientDisclosureTypeScCode":
                    self.loc_ê°„í¸ê³ ì§€ = i
                elif attr_name == "ZA_DisclosureTypeScCode":
                    self.loc_í†µí•©ê³ ì§€ = i
                elif attr_name == "FetusFlag":
                    self.loc_íƒœì•„êµ¬ë¶„ = i
                elif attr_name == "ZA_CoveragePaymentInprd":
                    self.loc_ë³´ê¸°ë‚©ê¸° = i
                elif attr_name == "ë‹´ë³´ê·¸ë£¹":
                    self.loc_ë‹´ë³´ê·¸ë£¹ = i
        
        # Find column locations in ë³´ì¥êµ¬ì¡°
        if self.data.arr_ë³´ì¥êµ¬ì¡° is not None and len(self.data.arr_ë³´ì¥êµ¬ì¡°) > 0:
            header_row = self.data.arr_ë³´ì¥êµ¬ì¡°[0]
            
            for i, val in enumerate(header_row):
                attr_name = str(val or "").strip().replace("\n", "")
                
                if attr_name == "ë³´ì¥ë°°ìˆ˜":
                    self.loc_ë³´ì¥ë°°ìˆ˜ = i
                elif attr_name == "ì„¸ë¶€ë‹´ë³´ìˆœë²ˆ":
                    self.loc_ì„¸ë¶€ë‹´ë³´ìˆœë²ˆ = i
                elif attr_name == "ì„¸ë¶€ë‹´ë³´ì½”ë“œ":
                    self.loc_ì„¸ë¶€ë‹´ë³´ì½”ë“œ = i
                elif attr_name == "íƒˆí‡´ìœ¨ê°œìˆ˜":
                    if self.loc_ê¸‰ë¶€íƒˆí‡´ìœ¨ê°œìˆ˜ == 0:
                        self.loc_ê¸‰ë¶€íƒˆí‡´ìœ¨ê°œìˆ˜ = i
                    else:
                        self.loc_ë‚©ë©´íƒˆí‡´ìœ¨ê°œìˆ˜ = i
        
        # Find column locations in ë³´ì¥ë°°ìˆ˜
        if self.data.arr_ë³´ì¥ë°°ìˆ˜ is not None and len(self.data.arr_ë³´ì¥ë°°ìˆ˜) > 1:
            for i in range(len(self.data.arr_ë³´ì¥ë°°ìˆ˜[1])):
                attr_name = str(self.data.arr_ë³´ì¥ë°°ìˆ˜[1][i] or "").strip().replace("\n", "")
                if not attr_name and len(self.data.arr_ë³´ì¥ë°°ìˆ˜) > 0:
                    attr_name = str(self.data.arr_ë³´ì¥ë°°ìˆ˜[0][i] or "").strip().replace("\n", "")
                
                if attr_name == "ì§€ê¸‰ë¥ ":
                    self.loc_ì§€ê¸‰ë¥  = i
                elif attr_name == "ì§€ê¸‰ì°¨ë…„":
                    self.loc_ë¶„í• ì§€ê¸‰ì°¨ë…„ = i
                elif attr_name == "ì—°ì§€ê¸‰íšŸìˆ˜":
                    self.loc_ì—°ì§€ê¸‰íšŸìˆ˜ = i
                elif attr_name == "ë©´ì±…ê¸°ê°„":
                    self.loc_ë©´ì±…ê¸°ê°„ = i
                elif attr_name == "ê°ì•¡ê¸°ê°„":
                    if self.loc_ê°ì•¡ê¸°ê°„ == 0:
                        self.loc_ê°ì•¡ê¸°ê°„ = i
                    else:
                        self.loc_ê°ì•¡ê¸°ê°„2 = i
                elif attr_name == "ê°ì•¡ë¹„ìœ¨":
                    if self.loc_ê°ì•¡ë¹„ìœ¨ == 0:
                        self.loc_ê°ì•¡ë¹„ìœ¨ = i
                    else:
                        self.loc_ê°ì•¡ë¹„ìœ¨2 = i
                elif attr_name == "15ì„¸ë¯¸ë§Œë©´ì±…ì ìš©":
                    self.loc_15ì„¸ë©´ì±… = i
        
        if log_callback:
            log_callback(f"ì»¬ëŸ¼ ìœ„ì¹˜ ë¶„ì„ ì™„ë£Œ: í™•ì¥ë²ˆí˜¸={self.loc_í™•ì¥ë²ˆí˜¸}, ë³´ê¸°ë‚©ê¸°={self.loc_ë³´ê¸°ë‚©ê¸°}")

    def _read_pgm_loop(self, dambo_code: str, dambo_att: DamboAttributes, loop_point: int, log_callback=None):
        """
        Read PGM data for each coverage code.
        Uses numpy arrays for fast lookup (nanosecond access).
        """
        try:
            # Initialize
            self.m_point = 0
            self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ = -1
            self.ì„¸ë¶€ë‹´ë³´ì½”ë“œList = []
            self.isê°ì•¡ë‘ë²ˆ = False
            self.min_riskrate_row = 0
            self.is_ë…íŠ¹ê°„í¸ = False
            
            # ===== M_Point Search (using INDEX - O(1)!) =====
            arr_main = self.data.arr_pgm_main
            
            if arr_main is not None:
                if self.data.ë…ë¦½íŠ¹ì•½ == 0:
                    # ì¼ë°˜ ë‹´ë³´: col1 ê¸°ì¤€ ê²€ìƒ‰
                    candidates = getattr(self.data, 'pgm_main_index_col1', {}).get(dambo_code, [])
                    if candidates:
                        self.m_point = candidates[0]
                else:
                    # ë…ë¦½íŠ¹ì•½: col0 ê¸°ì¤€ + í˜•êµ¬ë¶„ í•„í„°
                    candidates = getattr(self.data, 'pgm_main_index_col0', {}).get(dambo_code, [])
                    for idx in candidates:
                        row = arr_main[idx]
                        row_í˜•êµ¬ë¶„ = str(row[self.loc_í™•ì¥ë²ˆí˜¸] if len(row) > self.loc_í™•ì¥ë²ˆí˜¸ else "").strip()
                        if row_í˜•êµ¬ë¶„ == dambo_att.í˜•êµ¬ë¶„:
                            self.m_point = idx
                            break
            
            if self.m_point == 0:
                # if log_callback:
                #     log_callback(f"[ê²½ê³ ] M_Pointë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dambo_code}")
                return
            
            # ===== ë³´ì¥êµ¬ì¡° Lookup Key =====
            if self.data.ë…ë¦½íŠ¹ì•½ == 0:
                if self.m_point < len(arr_main) and self.loc_í™•ì¥ë²ˆí˜¸ < len(arr_main[self.m_point]):
                    dambo = arr_main[self.m_point][self.loc_í™•ì¥ë²ˆí˜¸]
                    ë³´ì¥êµ¬ì¡°_lookup_key = f"{self.data.product_code}_{dambo}"
                else:
                    ë³´ì¥êµ¬ì¡°_lookup_key = ""
            else:
                ë³´ì¥êµ¬ì¡°_lookup_key = f"{dambo_code}_{dambo_att.í˜•êµ¬ë¶„}"
                if self.loc_ë‹´ë³´ê·¸ë£¹ > 0 and self.m_point < len(arr_main):
                    self.ë…íŠ¹ë‹´ë³´ê·¸ë£¹ = arr_main[self.m_point][-1]  # Last column
            
            # ===== ì„¸ë¶€ë‹´ë³´ ê°œìˆ˜/ì½”ë“œ (using INDEX - O(1)!) =====
            arr_êµ¬ì¡° = self.data.arr_ë³´ì¥êµ¬ì¡°
            
            if arr_êµ¬ì¡° is not None and ë³´ì¥êµ¬ì¡°_lookup_key:
                matching_rows = getattr(self.data, 'ë³´ì¥êµ¬ì¡°_index', {}).get(ë³´ì¥êµ¬ì¡°_lookup_key, [])
                for index_lookup in matching_rows:
                    try:
                        if self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ <= 0:
                            self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ += 1
                        elif arr_êµ¬ì¡°[index_lookup][self.loc_ì„¸ë¶€ë‹´ë³´ìˆœë²ˆ] > self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜:
                            self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ += 1
                        else:
                            break
                        
                        if self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ == 0:
                            self.min_riskrate_row = index_lookup
                        
                        # ì„¸ë¶€ë‹´ë³´ì½”ë“œ ì €ì¥
                        if self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ >= 0 and self.loc_ì„¸ë¶€ë‹´ë³´ì½”ë“œ < len(arr_êµ¬ì¡°[index_lookup]):
                            self.ì„¸ë¶€ë‹´ë³´ì½”ë“œList.append(arr_êµ¬ì¡°[index_lookup][self.loc_ì„¸ë¶€ë‹´ë³´ì½”ë“œ])
                    except:
                        continue
            
            # ===== ë‚©ì…ë©´ì œì—¬ë¶€ =====
            if arr_êµ¬ì¡° is not None and self.min_riskrate_row < len(arr_êµ¬ì¡°) and self.loc_ë‚©ë©´íƒˆí‡´ìœ¨ê°œìˆ˜ > 0:
                try:
                    if arr_êµ¬ì¡°[self.min_riskrate_row][self.loc_ë‚©ë©´íƒˆí‡´ìœ¨ê°œìˆ˜] > 0:
                        self.ë‚©ì…ë©´ì œì—¬ë¶€ = True
                    else:
                        self.ë‚©ì…ë©´ì œì—¬ë¶€ = False
                except:
                    self.ë‚©ì…ë©´ì œì—¬ë¶€ = False
            
            # ===== ì„¸ë¶€ë‹´ë³´_bencoef ë°°ì—´ ìƒì„± =====
            arr_ë°°ìˆ˜ = self.data.arr_ë³´ì¥ë°°ìˆ˜
            
            if self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜ > 0 and arr_êµ¬ì¡° is not None and arr_ë°°ìˆ˜ is not None:
                self.ì„¸ë¶€ë‹´ë³´_bencoef = []
                for i in range(self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜):
                    try:
                        if self.min_riskrate_row + i + 1 < len(arr_êµ¬ì¡°):
                            ë³´ì¥ë°°ìˆ˜_key = arr_êµ¬ì¡°[self.min_riskrate_row + i + 1][self.loc_ë³´ì¥ë°°ìˆ˜]
                            bencoef_row = self.data.get_array_row(arr_ë°°ìˆ˜, ë³´ì¥ë°°ìˆ˜_key, 0)
                            if bencoef_row is not None:
                                self.ì„¸ë¶€ë‹´ë³´_bencoef.append([0] + list(bencoef_row))
                            else:
                                self.ì„¸ë¶€ë‹´ë³´_bencoef.append([0])
                        else:
                            self.ì„¸ë¶€ë‹´ë³´_bencoef.append([0])
                    except:
                        self.ì„¸ë¶€ë‹´ë³´_bencoef.append([0])
            
            # ===== ë©´ì±…/ê°ì•¡ Summary =====
            self.sum_ë©´ì±… = 0
            self.sum_ê°ì•¡ = 0
            
            for i in range(self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜):
                try:
                    if len(self.ì„¸ë¶€ë‹´ë³´_bencoef[i]) > self.loc_ë©´ì±…ê¸°ê°„:
                        val = self.ì„¸ë¶€ë‹´ë³´_bencoef[i][self.loc_ë©´ì±…ê¸°ê°„]
                        self.sum_ë©´ì±… += float(val or 0)
                    if len(self.ì„¸ë¶€ë‹´ë³´_bencoef[i]) > self.loc_ê°ì•¡ê¸°ê°„:
                        val = self.ì„¸ë¶€ë‹´ë³´_bencoef[i][self.loc_ê°ì•¡ê¸°ê°„]
                        self.sum_ê°ì•¡ += float(val or 0)
                except:
                    pass
            
            # ===== ê°ì•¡íšŸìˆ˜ =====
            for i in range(self.ì„¸ë¶€ë‹´ë³´ê°œìˆ˜):
                try:
                    if (len(self.ì„¸ë¶€ë‹´ë³´_bencoef[i]) > self.loc_ê°ì•¡ê¸°ê°„2 and 
                        len(self.ì„¸ë¶€ë‹´ë³´_bencoef[i]) > self.loc_ê°ì•¡ë¹„ìœ¨2):
                        if (self.ì„¸ë¶€ë‹´ë³´_bencoef[i][self.loc_ê°ì•¡ê¸°ê°„2] > 0 and 
                            self.ì„¸ë¶€ë‹´ë³´_bencoef[i][self.loc_ê°ì•¡ë¹„ìœ¨2] > 0):
                            self.isê°ì•¡ë‘ë²ˆ = True
                            break
                except:
                    pass
            
            # ===== ë³´ê¸°ë‚©ê¸° ì²˜ë¦¬ =====
            self._read_pgm_ë³´ê¸°ë‚©ê¸°(dambo_att, log_callback)
            
            # ===== ì„¸ë¶€ë³´ì¥ëª…_list ì—…ë°ì´íŠ¸ (PGM êµ¬ì¡° ìš°ì„ ) =====
            # PGMì—ì„œ ì„¸ë¶€ë‹´ë³´ì½”ë“œê°€ ë°œê²¬ë˜ì—ˆë‹¤ë©´, ì´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì„¸ë¶€ë³´ì¥ëª…_listì— ë°˜ì˜
            if self.ì„¸ë¶€ë‹´ë³´ì½”ë“œList and self.data.arr_ë‹´ë³´ë§¤í•‘ is not None:
                new_sembo_list = []
                # ë‹´ë³´ë§¤í•‘ì—ì„œ ì½”ë“œ->ëª…ì¹­ ì¡°íšŒ (Fast lookup using numpy)
                # ë‹´ë³´ë§¤í•‘ êµ¬ì¡°: [ëŒ€í‘œë‹´ë³´ì½”ë“œ, ëŒ€í‘œë‹´ë³´ëª…, ë‹´ë³´ì½”ë“œ, ë‹´ë³´ëª…, ...]
                # Index 2: ë‹´ë³´ì½”ë“œ, Index 3: ë‹´ë³´ëª…
                
                arr_mapping = self.data.arr_ë‹´ë³´ë§¤í•‘
                
                for sembo_code in self.ì„¸ë¶€ë‹´ë³´ì½”ë“œList:
                    try:
                        # Find row where col[2] == sembo_code
                        row = self.data.get_array_row(arr_mapping, sembo_code, 2)
                        if row is not None and len(row) > 3:
                            sembo_name = str(row[3] or "").strip()
                            if sembo_name:
                                new_sembo_list.append(sembo_name)
                            else:
                                new_sembo_list.append(sembo_code) # ì´ë¦„ì—†ìœ¼ë©´ ì½”ë“œë¼ë„
                        else:
                             # ë§¤í•‘ ì‹¤íŒ¨ ì‹œ ì½”ë“œ ìì²´ ì‚¬ìš© or Skip?
                             # ë³´í†µ ë§¤í•‘ì´ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë³´ë‹¤ ì½”ë“œê°€ ë‚˜ìŒ.
                             new_sembo_list.append(sembo_code)
                    except:
                        pass
                
                if new_sembo_list:
                    dambo_att.ì„¸ë¶€ë³´ì¥ëª…_list = new_sembo_list
                    # if log_callback:
                    #     log_callback(f"      ì„¸ë¶€ë³´ì¥ëª…_list ì—…ë°ì´íŠ¸(PGM): {new_sembo_list}")

        except Exception as e:
            if log_callback:
                log_callback(f"[ì—ëŸ¬] _read_pgm_loop: {e}")

    def _read_pgm_ë³´ê¸°ë‚©ê¸°(self, dambo_att: DamboAttributes, log_callback=None):
        """
        Read ë³´ê¸°ë‚©ê¸° data using numpy arrays.
        """
        try:
            self.n_array = []
            self.age_b = 999
            self.age_e = 0
            
            # TERM_Lookup_Key ìƒì„±
            arr_main = self.data.arr_pgm_main
            
            if arr_main is not None and self.m_point < len(arr_main):
                if self.data.ë…ë¦½íŠ¹ì•½ == 0:
                    if self.loc_ë³´ê¸°ë‚©ê¸° < len(arr_main[self.m_point]):
                        ë³´ê¸°ë‚©ê¸°ê°’ = arr_main[self.m_point][self.loc_ë³´ê¸°ë‚©ê¸°]
                        self.term_lookup_key = f"{self.data.product_code}_{ë³´ê¸°ë‚©ê¸°ê°’}"
                    else:
                        self.term_lookup_key = ""
                else:
                    if self.loc_ë³´ê¸°ë‚©ê¸° < len(arr_main[self.m_point]):
                        ë³´ê¸°ë‚©ê¸°ê°’ = arr_main[self.m_point][self.loc_ë³´ê¸°ë‚©ê¸°]
                        self.term_lookup_key = str(ë³´ê¸°ë‚©ê¸°ê°’)
                    else:
                        self.term_lookup_key = ""
            
            # N ë°°ì—´ ì €ì¥ (using INDEX - O(1)!)
            arr_ë³´ê¸° = self.data.arr_ë³´ê¸°ë‚©ê¸°
            
            if arr_ë³´ê¸° is not None and self.term_lookup_key:
                matching_rows = getattr(self.data, 'ë³´ê¸°ë‚©ê¸°_index', {}).get(self.term_lookup_key, [])
                for index_lookup in matching_rows:
                    try:
                        # Age_B, Age_E ì €ì¥
                        age_b_val = arr_ë³´ê¸°[index_lookup][8] if len(arr_ë³´ê¸°[index_lookup]) > 8 else 0
                        age_e_val = arr_ë³´ê¸°[index_lookup][9] if len(arr_ë³´ê¸°[index_lookup]) > 9 else 0
                        
                        self.age_b = min(self.age_b, int(age_b_val) if age_b_val else 0)
                        self.age_e = max(self.age_e, int(age_e_val) if age_e_val else 0)
                        
                        # N ë°°ì—´ì— ì¶”ê°€
                        n_entry = [
                            arr_ë³´ê¸°[index_lookup][6] if len(arr_ë³´ê¸°[index_lookup]) > 6 else "",
                            arr_ë³´ê¸°[index_lookup][11] if len(arr_ë³´ê¸°[index_lookup]) > 11 else "",
                            arr_ë³´ê¸°[index_lookup][12] if len(arr_ë³´ê¸°[index_lookup]) > 12 else ""
                        ]
                        self.n_array.append(n_entry)
                    except:
                        continue
            
            if self.age_b == 999:
                self.age_b = 0
                
        except Exception as e:
            if log_callback:
                log_callback(f"[ì—ëŸ¬] _read_pgm_ë³´ê¸°ë‚©ê¸°: {e}")

    def _set_ë…íŠ¹_file_path(self, dambo_att: DamboAttributes, ëŒ€í‘œë‹´ë³´ì½”ë“œ: str, dambo_code: str, log_callback=None):
        """
        ë…ë¦½íŠ¹ì•½ íŒŒì¼ ê²½ë¡œ ì¬ì§€ì • ë° ê°ì•¡ë©´ì±…ì—¬ë¶€ ì €ì¥.
        Uses numpy arrays for fast lookup.
        """
        try:
            # íŒŒì¼ê²½ë¡œ ì¬ì§€ì •
            arr_íŒŒì¼ëª… = self.data.arr_ë…íŠ¹íŒŒì¼ëª…
            if arr_íŒŒì¼ëª… is not None:
                for i in range(len(arr_íŒŒì¼ëª…)):
                    if arr_íŒŒì¼ëª…[i][0] == self.ë…íŠ¹ë‹´ë³´ê·¸ë£¹:
                        if self.is_ë…íŠ¹ê°„í¸:
                            self.data.ì•½ê´€íŒŒì¼ëª… = str(arr_íŒŒì¼ëª…[i][2])
                        else:
                            self.data.ì•½ê´€íŒŒì¼ëª… = str(arr_íŒŒì¼ëª…[i][1])
                        
                        # ë…ë¦½íŠ¹ì•½ëª… ì¶”ì¶œ
                        ì•½ê´€íŒŒì¼ëª… = self.data.ì•½ê´€íŒŒì¼ëª…
                        if ì•½ê´€íŒŒì¼ëª…:
                            start_idx = ì•½ê´€íŒŒì¼ëª….find("ì•½ê´€_")
                            if start_idx >= 0:
                                end_idx = ì•½ê´€íŒŒì¼ëª….find("_", start_idx + 3)
                                if end_idx > start_idx:
                                    self.ë…ë¦½íŠ¹ì•½ëª… = ì•½ê´€íŒŒì¼ëª…[start_idx + 3:end_idx]
                        break
            
            # ê°ì•¡ë©´ì±…ì—¬ë¶€ ì €ì¥
            arr_ë ˆì´ì•„ì›ƒ = self.data.arr_ë…íŠ¹ë ˆì´ì•„ì›ƒ
            if arr_ë ˆì´ì•„ì›ƒ is not None:
                layout_key = f"{dambo_code}_{dambo_att.í˜•êµ¬ë¶„}"
                for i in range(len(arr_ë ˆì´ì•„ì›ƒ)):
                    if str(arr_ë ˆì´ì•„ì›ƒ[i][0]).strip() == layout_key:
                        if "X" in str(arr_ë ˆì´ì•„ì›ƒ[i][7]):
                            dambo_att.ë…íŠ¹ê°ì•¡ì—¬ë¶€ = "ê°ì•¡ì—†ìŒ"
                        else:
                            dambo_att.ë…íŠ¹ê°ì•¡ì—¬ë¶€ = "ê°ì•¡ìˆìŒ"
                        
                        if "X" in str(arr_ë ˆì´ì•„ì›ƒ[i][8]):
                            dambo_att.ë…íŠ¹ë©´ì±…ì—¬ë¶€ = "ë©´ì±…ì—†ìŒ"
                        else:
                            dambo_att.ë…íŠ¹ë©´ì±…ì—¬ë¶€ = "ë©´ì±…ìˆìŒ"
                        break
                        
        except Exception as e:
            if log_callback:
                log_callback(f"[ì—ëŸ¬] _set_ë…íŠ¹_file_path: {e}")

    def _build_comment_caches(self, log_callback=None):
        """
        Build comment caches from source and target documents ONCE.
        Eliminates repeated COM Comments traversal per coverage item.
        """
        import time
        cache_start = time.time()
        
        # ===== Cache source document comments =====
        # Structure: {source_type: [(ëŒ€í‘œë‹´ë³´ì½”ë“œ_list, copy_start, copy_end, íŠ¹ë³„ì•½ê´€ëª…), ...]}
        if hasattr(self, 'source_docs') and self.source_docs:
            for source_type, source_doc in self.source_docs.items():
                comment_list = []
                try:
                    for comment in source_doc.Comments:
                        comment_text = str(comment.Range.Text)
                        codes = [c.strip() for c in comment_text.split(",")]
                        para = comment.Scope.Paragraphs(1)
                        para_start = para.Range.Start
                        íŠ¹ë³„ì•½ê´€ëª… = str(para.Range.Text).strip().replace('\r', '').replace('\n', '')
                        comment_list.append((codes, para_start, íŠ¹ë³„ì•½ê´€ëª…))
                except Exception as e:
                    if log_callback:
                        log_callback(f"   âš ï¸ ì†ŒìŠ¤ ì£¼ì„ ìºì‹œ ì˜¤ë¥˜ ({source_type}): {e}")
                
                # Build copy ranges: pair consecutive comments as start/end
                for i, (codes, para_start, íŠ¹ë³„ì•½ê´€ëª…) in enumerate(comment_list):
                    # End position is the start of the next comment's paragraph
                    if i + 1 < len(comment_list):
                        copy_end = comment_list[i + 1][1]  # next comment's para_start
                    else:
                        copy_end = para_start  # last comment - no range
                    
                    for code in codes:
                        if code not in self._source_comment_cache:
                            self._source_comment_cache[code] = {}
                        self._source_comment_cache[code][source_type] = (para_start, copy_end, íŠ¹ë³„ì•½ê´€ëª…)
        
        # ===== Cache target document comment OBJECTS (not positions!) =====
        # COM objects auto-update their positions when document content changes
        target_doc = self.word.target_doc
        if target_doc:
            try:
                for comment in target_doc.Comments:
                    comment_text = str(comment.Range.Text)
                    self._target_comment_objects.append((comment_text, comment))
            except Exception as e:
                if log_callback:
                    log_callback(f"   âš ï¸ íƒ€ê²Ÿ ì£¼ì„ ìºì‹œ ì˜¤ë¥˜: {e}")
        
        cache_time = time.time() - cache_start
        
        if log_callback:
            log_callback(f"âš¡ ì£¼ì„ ìºì‹œ êµ¬ì¶• ì™„ë£Œ: ì†ŒìŠ¤ {len(self._source_comment_cache)}ê°œ, "
                        f"íƒ€ê²Ÿ {len(self._target_comment_objects)}ê°œ ({cache_time:.2f}ì´ˆ)")

    def _copy_terms(self, ëŒ€í‘œë‹´ë³´ì½”ë“œ: str, íŠ¹ë³„ì•½ê´€ê²½ë¡œ: str, íŠ¹ë³„ì•½ê´€íŒŒì¼ëª…: str, 
                    dambo_category: str, dambo_att: DamboAttributes, log_callback=None):
        """
        Copy terms from source document to target document.
        Uses pre-built comment cache for O(1) lookup instead of iterating all comments.
        """
        try:
            # Use pre-cached source document (opened before main loop)
            source_doc = None
            if hasattr(self, 'source_docs') and self.source_docs:
                source_doc = self.source_docs.get(dambo_category)
                if not source_doc and self.source_docs:
                    source_doc = list(self.source_docs.values())[0]
            
            if not source_doc:
                if log_callback:
                    log_callback(f"   âš ï¸ ì†ŒìŠ¤ ë¬¸ì„œ ì—†ìŒ: {dambo_category}")
                return
            
            target_doc = self.word.target_doc
            if not target_doc:
                if log_callback:
                    log_callback(f"   âš ï¸ íƒ€ê²Ÿ ë¬¸ì„œ ì—†ìŒ")
                return
            
            # ===== CACHED LOOKUP (O(1)) instead of iterating all comments =====
            cache_entry = self._source_comment_cache.get(ëŒ€í‘œë‹´ë³´ì½”ë“œ, {})
            source_info = cache_entry.get(dambo_category)
            if not source_info and cache_entry:
                # Fallback to first available source type
                source_info = list(cache_entry.values())[0]
            
            if not source_info:
                if log_callback:
                    log_callback(f"   âš ï¸ ë³µì‚¬ ë²”ìœ„ ì—†ìŒ (ìºì‹œ): {ëŒ€í‘œë‹´ë³´ì½”ë“œ}")
                return None
            
            copy_start, copy_end, íŠ¹ë³„ì•½ê´€ëª… = source_info
            
            # Check if already copied
            if íŠ¹ë³„ì•½ê´€ëª… in self.copied_list_strings:
                if log_callback:
                    log_callback(f"   â­ï¸ ì´ë¯¸ ë³µì‚¬ë¨: {íŠ¹ë³„ì•½ê´€ëª…}")
                return None
            
            self.copied_list_strings.add(íŠ¹ë³„ì•½ê´€ëª…)
            
            if copy_start >= copy_end:
                if log_callback:
                    log_callback(f"   âš ï¸ ë³µì‚¬ ë²”ìœ„ ì—†ìŒ: {ëŒ€í‘œë‹´ë³´ì½”ë“œ}")
                return None
            
            copy_range = source_doc.Range(copy_start, copy_end)
            
            # Find paste location using cached section positions
            paste_range = self._find_paste_location(target_doc, dambo_category, dambo_att, log_callback)
            
            if paste_range:
                paste_range.InsertAfter("\r")
                paste_range.Collapse(0)  # wdCollapseEnd
                
                start_pos = paste_range.Start
                paste_range.FormattedText = copy_range.FormattedText
                end_pos = paste_range.End
                
                if end_pos <= start_pos:
                    try:
                        end_pos = paste_range.Paragraphs(1).Range.End
                    except:
                        pass
                
                try:
                    if end_pos <= start_pos + 5:
                        result_range = self._recalculate_paste_range_end(target_doc, start_pos, log_callback)
                    else:
                        result_range = target_doc.Range(start_pos, end_pos)
                except Exception as e:
                    result_range = paste_range
                
                return result_range
            else:
                if log_callback:
                    log_callback(f"   âš ï¸ ë¶™ì—¬ë„£ê¸° ìœ„ì¹˜ ì—†ìŒ: {ëŒ€í‘œë‹´ë³´ì½”ë“œ}")
            
            return None
                    
        except Exception as e:
            if log_callback:
                log_callback(f"   âŒ ì•½ê´€ ë³µì‚¬ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()

    def _find_paste_location(self, target_doc, dambo_category: str, dambo_att: DamboAttributes, log_callback=None):
        """
        Find paste location in target document - at the END of the section.
        Uses cached COM Comment objects (positions auto-update).
        """
        ëª¨ë“ˆí˜•_val = getattr(self.data, 'ëª¨ë“ˆí˜•', 0)
        
        if (ëª¨ë“ˆí˜•_val == 1 or dambo_att.ëª¨ë“ˆ) and dambo_att.ëª¨ë“ˆ:
            section_title = f"{dambo_att.ëª¨ë“ˆ}-{dambo_category}ê´€ë ¨ íŠ¹ë³„ì•½ê´€"
        else:
            section_title = f"{dambo_category}ê´€ë ¨ íŠ¹ë³„ì•½ê´€"
        
        # Search in cached comment objects (positions auto-update via COM)
        section_markers = ["ê´€ë ¨ íŠ¹ë³„ì•½ê´€", "ì œë„ì„± íŠ¹ë³„ì•½ê´€", "ë³„í‘œ"]
        
        for i, (comment_text, comment_obj) in enumerate(self._target_comment_objects):
            if section_title in comment_text:
                # Found section start - get FRESH position from COM object
                try:
                    section_start_pos = comment_obj.Scope.Paragraphs(1).Range.Start
                except:
                    continue
                
                # Find next section marker for end position
                section_end_pos = None
                for j in range(i + 1, len(self._target_comment_objects)):
                    next_text = self._target_comment_objects[j][0]
                    if any(marker in next_text for marker in section_markers):
                        if comment_text not in next_text:  # Different section
                            try:
                                section_end_pos = self._target_comment_objects[j][1].Scope.Paragraphs(1).Range.Start - 1
                            except:
                                pass
                            break
                
                if section_end_pos is not None:
                    return target_doc.Range(section_start_pos, section_end_pos)
                else:
                    return target_doc.Range(section_start_pos, section_start_pos)
        
        return None

    def _revise_terms(self, dambo_att: DamboAttributes, loop_point: int, target_range, log_callback=None):
        """
        Revise terms with specific attribute values using TagProcessor.
        Handles substitution tags and output control tags within the specified range.
        
        Args:
            dambo_att: DamboAttributes
            loop_point: Loop index
            target_range: Word Range object to process (e.g., pasted text)
        """
        try:
            # Create TagContext from DamboAttributes
            context = create_tag_context_from_dambo_att(dambo_att, self.data)

            # Set additional context from PGM data
            context.ë©´ì±… = 1 if self.sum_ë©´ì±… > 0 else 0
            context.ê°ì•¡ = 1 if self.sum_ê°ì•¡ > 0 else 0
            context.ê°ì•¡í•œë²ˆ = not self.isê°ì•¡ë‘ë²ˆ
            context.ê°ì•¡ë‘ë²ˆ = self.isê°ì•¡ë‘ë²ˆ
            context.ë…ë¦½íŠ¹ì•½ = self.data.ë…ë¦½íŠ¹ì•½
            context.ë¹„ê°±ì‹  = 1 if self.data.ìë™ê°±ì‹ í˜• == 0 else 0
            
            # Build ê°ì•¡ê¸°ê°„ list from PGM
            if self.ì„¸ë¶€ë‹´ë³´_bencoef:
                for bencoef in self.ì„¸ë¶€ë‹´ë³´_bencoef:
                    ê¸°ê°„_list = []
                    # Check safe access
                    if len(bencoef) > self.loc_ê°ì•¡ê¸°ê°„ and bencoef[self.loc_ê°ì•¡ê¸°ê°„]:
                        try:
                            ê¸°ê°„_list.append(int(bencoef[self.loc_ê°ì•¡ê¸°ê°„]))
                        except (ValueError, TypeError):
                            pass
                    if len(bencoef) > self.loc_ê°ì•¡ê¸°ê°„2 and bencoef[self.loc_ê°ì•¡ê¸°ê°„2]:
                        try:
                            ê¸°ê°„_list.append(int(bencoef[self.loc_ê°ì•¡ê¸°ê°„2]))
                        except (ValueError, TypeError):
                            pass
                    
                    if ê¸°ê°„_list:
                        context.ê°ì•¡ê¸°ê°„_list.append(ê¸°ê°„_list)
            
            # Build ì§€ê¸‰ë¥  data from PGM
            if self.ì„¸ë¶€ë‹´ë³´_bencoef and context.ëŒ€í‘œë‹´ë³´ì½”ë“œ:
                ì§€ê¸‰ë¥ _list = []
                for bencoef in self.ì„¸ë¶€ë‹´ë³´_bencoef:
                    if len(bencoef) > self.loc_ì§€ê¸‰ë¥  and bencoef[self.loc_ì§€ê¸‰ë¥ ]:
                        try:
                            ì§€ê¸‰ë¥ _list.append([float(bencoef[self.loc_ì§€ê¸‰ë¥ ])])
                        except (ValueError, TypeError):
                            ì§€ê¸‰ë¥ _list.append([100.0])
                    else:
                        ì§€ê¸‰ë¥ _list.append([100.0])
                context.ì§€ê¸‰ë¥ _data[context.ëŒ€í‘œë‹´ë³´ì½”ë“œ] = ì§€ê¸‰ë¥ _list
            
            # Process tags in target RANGE
            if target_range:
                self.tag_processor.csv_loader = self.csv_loader if hasattr(self, 'csv_loader') else None
                self.tag_processor.process_range(target_range, context, log_callback)
            else:
                if log_callback:
                    log_callback(f"   âš ï¸ target_range ì—†ìŒ - íƒœê·¸ ì²˜ë¦¬ ìŠ¤í‚µ")
                
        except Exception as e:
            if log_callback:
                log_callback(f"   âŒ [ì—ëŸ¬] _revise_terms: {e}")
            import traceback
            traceback.print_exc()

    def _revise_ë³„í‘œ_0ì„¸(self, log_callback=None):
        """
        Handle 0ì„¸ (zero age) child related revisions.
        Processes 0ì„¸ìë…€ related tags.
        """
        try:
            if self.data.zero_age_ìë…€ == 0:
                return  # No 0ì„¸ìë…€ processing needed
            
            if log_callback:
                log_callback("0ì„¸ìë…€ íƒœê·¸ ì²˜ë¦¬ ì¤‘...")
            
            # Process 0ì„¸ìë…€ specific tags
            # TODO: Implement specific 0ì„¸ìë…€ tag processing
            
            if log_callback:
                log_callback("0ì„¸ìë…€ íƒœê·¸ ì²˜ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            if log_callback:
                log_callback(f"[ì—ëŸ¬] _revise_ë³„í‘œ_0ì„¸: {e}")
    def _recalculate_paste_range_end(self, target_doc, start_pos, log_callback=None):
        """
        Calculates the end position of the pasted content by finding the next section or end of document.
        """
        try:
            # Search for the next section marker starting from start_pos
            search_range = target_doc.Range(start_pos, target_doc.Content.End)
            
            # Find next section or end of doc
            search_range.Find.ClearFormatting()
            found = False
            
            # Markers that indicate start of a new section
            markers = ["ê´€ë ¨ íŠ¹ë³„ì•½ê´€", "ì œë„ì„± íŠ¹ë³„ì•½ê´€", "ë³„í‘œ"]
            
            min_end_pos = target_doc.Content.End
            
            for marker in markers:
                search_range.Find.Execute(FindText=marker, Forward=True, Wrap=0)
                if search_range.Find.Found:
                    # Found a marker. The end of our section is before this marker's paragraph.
                    # Move to start of the paragraph containing the marker
                    marker_para_start = search_range.Paragraphs(1).Range.Start
                    if marker_para_start < min_end_pos and marker_para_start > start_pos:
                        min_end_pos = marker_para_start
                        found = True
                
                # Reset search range for next marker check
                search_range = target_doc.Range(start_pos, target_doc.Content.End)
            
            return target_doc.Range(start_pos, min_end_pos)
            
        except Exception as e:
            if log_callback:
                log_callback(f"   âš ï¸ Range calculation error: {e}")
            return target_doc.Range(start_pos, target_doc.Content.End)
